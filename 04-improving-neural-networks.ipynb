{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left; margin-bottom: 1em\" src=\"images/excelerate.png\" width=\"200\">\n",
    "<img style=\"float: right; margin-bottom: 1em\" src=\"images/surfsara.png\" width=\"150\">\n",
    "<hr style=\"clear: both\"/>\n",
    "\n",
    "# Improving neural networks\n",
    "In this notebook you will work on improving a neural network on a new data set. This data set is taken from the [PCam](https://github.com/basveeling/pcam) dataset, and consists of a thousand patches taken from a much larger histopathological slices of lymph node sections.\n",
    "\n",
    "Each patch will either have metastatic (cancerous) tissue or not. As in the previous notebooks, your task is to train a network that will solve this classification task.\n",
    "\n",
    "Let's load the data, and plot the first 16 images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib\n",
    "import keras\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "X, Y, labels = lib.dataset_pcam()\n",
    "\n",
    "X = X[:1000]\n",
    "Y = Y[:1000]\n",
    "\n",
    "print('Data set size: {}'.format(X.shape))\n",
    "\n",
    "lib.plot_examples(X[:16], Y[:16], labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a dense network to serve as a performance baseline. We can use it to compare to more advanced architectures like CNNs. Run the next cell and take note of the final accuracy on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=X.shape[1:]))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(Adam(lr=0.000001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X, Y, validation_split=0.2, epochs=25, batch_size=32)\n",
    "lib.plot_history(history);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "Now implement a CNN to solve the problem by filling out the skeleton below.\n",
    "\n",
    "How does a CNN perform on this dataset compared with a dense network? Why do you think it performs worse or better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=X.shape[1:]))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Add additional conv and max pool layers here\n",
    "\n",
    "# Add one or more dense layers here\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(Adam(lr=<FILL IN>), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X, Y, validation_split=0.2, epochs=25, batch_size=32)\n",
    "lib.plot_history(history);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE FILL IN YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Inspect the class activation maps for the different layers by running the next cell. Do they make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_index = 1\n",
    "lib.plot_cam(model, X[example_index], np.argmax(Y[example_index]), labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Although the network is performing quite well, we can speed up convergence by adding **batch normalisation** layers. We add those between the convolutional layers and the max pooling layers.\n",
    "\n",
    "In the next cell we import the batch normalisation layer, called [`BatchNormalization`](https://keras.io/layers/normalization/) in Keras. Copy-paste the network you trained in exercise 3 into the cell below, and add the batch normalisation layers **between the `Conv2D` and `MaxPool2D` layers**.\n",
    "\n",
    "Rerun the training. How do the layers affect the convergence speed? Why do you think that is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization\n",
    "\n",
    "# COPY-PASTE YOUR NETWORK FROM EXERCISE 3 HERE, AND ADD THE BATCH NORMALISATION LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL IN YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "Look at the loss curves for the training and validation set in exercise 3. At what point does the model start overfitting? How can you tell?\n",
    "\n",
    "Motivate your answer in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL IN YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "Add dropout layers between the dense layers and train the network again. Use a fairly high rate (e.g. 0.5) of dropout and inspect the resulting validation and training loss curves. Does it have any effect? If so, how?\n",
    "\n",
    "We have imported the dropout layer (simply called [`Dropout`](https://keras.io/layers/core/#dropout) in Keras) for you in the first line of the next cell. Copy-paste your original network from exercise 3 and add the dropout layers.\n",
    "\n",
    "**Hint**: the dropout rate is a floating point number as the first parameter of `Dropout`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "# COPY-PASTE YOUR NETWORK FROM EXERCISE 3 HERE, AND ADD THE DROPOUT LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL IN YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "We have regularised the dense part of the network by using dropout. We can do the same for the convolutional layers by penalising large weights in the filter kernels. We do this by adding an l2 regulariser on the kernel weights.\n",
    "\n",
    "In the cell below we load the [`l2`](https://keras.io/regularizers) regulariser and add it to the first convolutional layer. For this we use the `kernel_regularizer` parameter in the `Conv2D` layer. In this example we set the strength of the regularization to 0.01. Depending on the data set and optimisation you may want to try different values.\n",
    "\n",
    "Apply l2 regularisation to the kernels of all convolutional layers of your network from exercise 5, and rerun the training process. How can you tell we more effectively control overfitting now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE:\n",
    "\n",
    "from keras.regularizers import l2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=X.shape[1:], kernel_regularizer=l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# COPY-PASTE THE REST OF YOUR NETWORK HERE, OR COPY THE kernel_regularizer PARAMETER TO YOUR OWN NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL IN YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9: bonus\n",
    "Increase the regularisation strength (e.g. by an order of magnitude, going from 0.01 to 0.1) and/or increase the dropout rate. What happens with the convergence? Why is that? Motivate your answer in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE COPY-PASTE YOUR NETWORK HERE FROM EXERCISE 8, AND MODIFY THE REGULARISATION STRENGTH AND/OR DROPOUT RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE FILL IN YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
